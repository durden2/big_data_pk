{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# classifiers\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "    \n",
    "def scaleData(data, withMinMaxScaller = False, withScaller = False):    \n",
    "    #data vector\n",
    "    scaled_features = data.drop('Outcome', axis=1)\n",
    "\n",
    "    #min max scaller\n",
    "    if withMinMaxScaller:\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(data.drop('Outcome', axis=1))\n",
    "        scaled_features = scaler.transform(data.drop('Outcome', axis=1))\n",
    "\n",
    "    #standard scaller\n",
    "\n",
    "    if withScaller:\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(data.drop('Outcome', axis=1))\n",
    "        scaled_features = scaler.transform(data.drop('Outcome', axis=1))\n",
    "        \n",
    "    return scaled_features\n",
    "\n",
    "def gridSearch(clf, params_grid, X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=121)\n",
    "    \n",
    "    grid = GridSearchCV(clf, params_grid, cv=5, n_jobs = -1, scoring = 'accuracy')\n",
    "\n",
    "    grid.fit(X_train,y_train)\n",
    "        \n",
    "    predictions = grid.predict(X_test)\n",
    "    \n",
    "    return {\n",
    "        'bestParams': grid.best_params_,\n",
    "        'score': accuracy_score(y_test, predictions)\n",
    "    }\n",
    "\n",
    "def classify(clf, data, params_grid = {}, withMinMaxScaller = False, withScaller = False):    \n",
    "    # k in kfold cross validation \n",
    "    k = 5\n",
    "\n",
    "    #construct data vector\n",
    "    scaled_features = scaleData(data, withMinMaxScaller, withScaller)\n",
    "    \n",
    "    #cross validation\n",
    "    X = scaled_features\n",
    "    y = data['Outcome']\n",
    "    \n",
    "    return gridSearch(clf, params_grid, X, y)\n",
    "    \n",
    "    ###Cross Validation\n",
    "    #     predictions = cross_val_predict(clf, X,y, cv=k)\n",
    "\n",
    "    #     # take mean of scores and multiply by 100 to get percents\n",
    "\n",
    "    #     return (cross_val_score(clf, X, y, scoring='accuracy', cv = k)).mean() * 100\n",
    "\n",
    "    ###Bez kross walidacji\n",
    "\n",
    "    #     X_train, X_test, y_train, y_test = train_test_split(\n",
    "    #         X, y, test_size=0.3, random_state=121)\n",
    "\n",
    "    #     clf.fit(X_train, y_train)\n",
    "    #     predictions = clf.predict(X_test)\n",
    "    #     return accuracy_score(y_test, predictions)\n",
    "\n",
    "def test_classifiers(data, withScaller=False, withMinMaxScaller=False):\n",
    "    scores = {}\n",
    "\n",
    "    classifiers = [\n",
    "        SVC(gamma = 'auto'),\n",
    "        KNeighborsClassifier(n_neighbors=5),\n",
    "        DecisionTreeClassifier(),\n",
    "        RandomForestClassifier(),\n",
    "        LogisticRegression(solver='liblinear'),\n",
    "        GaussianNB(),\n",
    "        AdaBoostClassifier(),        \n",
    "    ]\n",
    "    \n",
    "    classNames = [\n",
    "        'SVC',\n",
    "        'KNN',\n",
    "        'DecisionTree',\n",
    "        'RandomForest',\n",
    "        'LogisticRegression',\n",
    "        'GaussianNaive',\n",
    "        'AdaBoost',\n",
    "    ]\n",
    "\n",
    "    param_grid = {\n",
    "        'SVC': {\n",
    "            'kernel': ['linear', 'rbf'],\n",
    "            'gamma': ['auto', 0.01, 0.1, 0.5, 1, 2, 10],\n",
    "            'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "        },\n",
    "        'KNN': {\n",
    "            'n_neighbors': list(range(1,5)),\n",
    "            'weights': [\"uniform\", \"distance\"],\n",
    "        },\n",
    "        'DecisionTree': {\n",
    "            'max_depth': np.arange(3, 10),\n",
    "        },\n",
    "        'LogisticRegression': {\n",
    "            'C': np.logspace(-3,3,7),\n",
    "            'penalty': ['l1','l2'],\n",
    "        },\n",
    "        # has no hyperparams\n",
    "        'GaussianNaive': {},\n",
    "        'AdaBoost': {},\n",
    "        'RandomForest': {\n",
    "            \"max_depth\": [2, 3, None],\n",
    "            \"max_features\": [1, 2, 6, 7],\n",
    "            \"min_samples_split\": [2, 3],\n",
    "            \"bootstrap\": [True, False],\n",
    "            \"criterion\": [\"gini\", \"entropy\"],\n",
    "            \"n_estimators\": [10, 100],\n",
    "        }\n",
    "    }     \n",
    "    for name, classif in zip(classNames, classifiers):\n",
    "        print('Calculate for: ' + name)\n",
    "        scores[name] = classify(classif, data, param_grid[name], withMinMaxScaller, withScaller)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate for: SVC\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "%matplotlib inline\n",
    "import json\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "# get all files in tests directory\n",
    "\n",
    "mypath = './tests'\n",
    "files = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "\n",
    "#files = ['diabetes_mean.csv']\n",
    "\n",
    "scores = []\n",
    "bestScores = []\n",
    "\n",
    "def getDiggedData(diction, dataToExtract):\n",
    "    finalScore = {}\n",
    "    for i in diction:\n",
    "        finalScore[i] = diction[i][dataToExtract]\n",
    "        \n",
    "    return finalScore\n",
    "\n",
    "   \n",
    "for i in files:\n",
    "    data1 = pd.read_csv('tests/' + i)    \n",
    "    scoresPerFile = []\n",
    "    \n",
    "    for scale in ['bez', 'std', 'minMax']:        \n",
    "        result = test_classifiers(data1, True if scale == 'std' else False)\n",
    "        localScore = {\n",
    "            'scaler': scale,\n",
    "            'clf': result,\n",
    "        }\n",
    "\n",
    "        scoresPerFile.append(localScore)\n",
    "        \n",
    "    fileScoresObj = {\n",
    "        'file': i,\n",
    "        'scores': scoresPerFile\n",
    "    }\n",
    "    \n",
    "    scores.append(fileScoresObj)   \n",
    "\n",
    "print(json.dumps(scores, indent=1))\n",
    "\n",
    "for i in scores:\n",
    "    print('Test dla pliku: ' + i['file'])\n",
    "    \n",
    "    for scores in i['scores']:\n",
    "        print(' przy uzyciu scalera: ' + scores['scaler'] + '\\n')\n",
    "        print('Najlepszy klasyfikator w zbiorze: ')\n",
    "        allClfScores = getDiggedData(scores['clf'], 'score')\n",
    "        best = max(allClfScores, key=allClfScores.get)\n",
    "        print(best + ' dopasowanie: ' + str(scores['clf'][best]['score']) + '\\n')\n",
    "        bestScores.append({\n",
    "            'file': i['file'],\n",
    "            'scaler': scores['scaler'],\n",
    "            'score': scores['clf'][best]['score'],\n",
    "            'params': getDiggedData(scores['clf'], 'bestParams')[best],\n",
    "        })\n",
    "        \n",
    "print('Najlepszy wynik: ')\n",
    "print(max(bestScores, key=lambda item: item['score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
